This is a Kafka producer load simulator designed to generate a realistic, sustained stream of IoT event data for development, testing, and performance benchmarking of a downstream data pipeline.

Here's the technical breakdown:


Core Purpose & Architecture:

Load Generation: It synthesizes a high-volume, continuous stream of two common IoT event types to simulate a production environment.


Decoupled Data Streams: It wisely separates events into two distinct Kafka topics (temperature_reads and price_lookups). This allows downstream consumers (e.g., time-series databases for temperature, product catalog services for price lookups) to subscribe only to the relevant data stream, following a best-practice microservices architecture.

The Data Models:


Temperature Event: Simulates data from a temperature sensor. It includes a unique device EPC (using the standardized format we discussed), a realistic temperature range, a unit, and a precise timestamp.

Price Lookup Event: Simulates a retail IoT interaction (e.g., from a smart shelf, RFID scanner, or digital kiosk). It triggers a lookup against a simulated product catalog and sends the entire product details.


Key Technical Features:

Asynchronous Operation: Uses asyncio and AIOKafkaProducer for high-performance, non-blocking I/O, enabling it to handle thousands of concurrent simulated devices efficiently.

Controlled Load: The SLEEP_MS and device-level timing (device_last_event) prevent a pure, system-breaking flood. It enforces a maximum event rate per device, creating a more realistic and sustainable load.

Configurability: All critical parameters (Kafka connection, number of devices, topics, sleep intervals) are driven by environment variables, making it highly adaptable for different test scenarios.

Resilience: Incorporates a retry mechanism with exponential backoff for initial Kafka connection failures, ensuring robustness in containerized or orchestrated environments where dependencies may not be immediately available.


In summary for the IoT Engineer: This script is not for production data ingestion. It's a foundational tool for pipeline development. It allows you to validate your Kafka infrastructure, test the performance and logic of your stream processors (like Kafka Streams or Flink jobs), and develop dashboard visualizations, all with a reliable and configurable source of mock data that accurately reflects your intended IoT device behavior.
